%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2011 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\E{{\bf E}}
\def\S{K}
\def\reals{\mathbb{R}}
\def\tr{\mathrm{tr}}
\def\cS{\mathcal{S}}
\def\cL{\mathcal{L}}
\def\U{\mathcal{U}}
\def\hp{\hat{p}}

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2011,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{algorithmic}

% As of 2010, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2011} with
% \usepackage[nohyperref]{icml2011} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2011}
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Appearing in''
% \usepackage[accepted]{icml2011}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Actively Learning the Crowd Kernel}

\begin{document}

\icmltitle{Supplementary Material: Capturing the Crowd Kernel}
%or Capturing the Crowd Kernel, Actively?


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2011
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{active learning, crowdsourcing, kernels}

\vskip 0.3in


\section{Relative regression}

We consider the following online relative regression model.  There is a sequence of examples $(x_1,x_1',y_1),(x_2,x_2',y_2),\ldots,(x_T,x_T',y_T) \in X \times X \times \{0,1\}$, for some set $X \subseteq \reals^d$.  For $w \in \reals^d$, define the relative linear model with $w$ to be,
$$p_t(w) = \frac{w \cdot x_t}{w \cdot x_t + w \cdot x_t'}.$$
The sequence $x_1,x_1',\ldots,x_T,x_T'$ is chosen arbitrary (or even adversarially) in advance; afterwards it is assumed that there is some $w^*\in \reals^d$ such that, $\Pr[y_t=1]=p_t(w^*)$ and that the different $y_t$'s are independent.  It is further assumed that $w^*$ belongs to some convex compact set $W \subset \reals^d$ and that $w \cdot x$ is positive and bounded over $w \in W,x\in X$.  Without loss of generality, by scaling we can require $w\cdot x \in [1,\beta]$ for some $\beta>0$ and every $w \in W,x\in X$.

On the $t$th period, the algorithm outputs $w_t \in W$, then observes $x_t,x_t',y_t$, and finally incurs loss $\ell_t(w_t)$ where $\ell_t(w)=\log 1/p_t(w)$ if $y_t=1$ and $\ell_t(w)=\log 1/(1-p_t(w))$ if $y_t=0$.  The goal of the algorithm is to incur total loss not much larger than $\sum_t \ell_t(w^*)$, the best choice had we known $w^*$ in advance.  

We note that an analogous (and slightly simpler version of) the following lemma can be proven for squared loss.
\begin{lemma}
Let $X,W \subseteq \reals^d$ and suppose that $W$ is compact and convex and $\exists \alpha >0$ such that for all $x \in X$, $w \in W$:
$\|x\|,\|w\|\leq 1$, and $w \cdot x \geq \alpha$.
The for $\eta=\sqrt{2\alpha/T}$ and any $w^0\in W$ and $w^{t+1}=\Pi_W(w^t-\eta \nabla \ell_t(w_t))$,
$$\E\left[\sum_{t=1}^T \ell_t(w_t) -\ell_t(w^*)\right] \leq \sqrt{\frac{8T}{\alpha^3}}.$$
\end{lemma}

\begin{proof}
Following the analysis of Zinkevich \cite{Zinkevich03} we consider the potential equal to the squared distance $(w_t-w^*)^2$ and argue that it decreases whenever we have substantial error.
Let $\nabla_t = \nabla \ell_t(w_t)\in \reals^d$, which is,
$$\nabla_t = \frac{x_t+x_t'}{w_t\cdot x_t+w_t\cdot x_t'} - y_t \frac{x_t}{w_t\cdot x_t} - (1-y_t)\frac{x_t'}{w_t\cdot x_t'}.$$
By the triangle inequality $\|\nabla_t\| \leq G$ for $G=\frac{2}{\alpha}$.  Now, as Zinkevich points out, due to convexity of $W$, $(w-\Pi_W(v))^2\leq (w-v)^2$ for any $v \in \reals^d$ and $w \in W$.  Hence,
$$(w^*-w_{t+1})^2 \leq (w^*-w_t+\eta \nabla_t)^2.$$
Thus the {\em decrease} in potential, call it $\Delta_t = (w^*-w_t)^2-(w^*-w_{t+1})^2$, is at least:
\begin{align*}
\Delta_t &\geq (w^*-w_t)^2-(w^*-w_t+\eta \nabla_t)^2 \\
&=2\eta \nabla_t \cdot (w_t-w^*)-\eta^2 \nabla_t^2.
\end{align*}

Next, we consider the quantity,
$\E[ \Delta_t \cdot w^* ]$, where the expectation is taken over the random $y_t$ (fixing $y_1,y_2,\ldots,y_{t-1}$).  By expansion, the expectations is:
$$\frac{w^* \cdot x_t + w^* \cdot x_t'}{w \cdot x_t + w\cdot x_t'} - p_t(w^*) \frac{w^* \cdot x_t}{w_t\cdot x_t} - (1-p_t(w^*))\frac{w^* \cdot x_t'}{w_t\cdot x_t'}.$$
After simple algebraic manipulation, which is difficult to show in two-column format, we have,
\begin{align*}
\E[\Delta_t \cdot w^*]&=-Z_t(p_t(w^*)-p_t(w_t))^2 \text{ where }\\
Z_t &=\frac{(w_t\cdot x_t+w_t\cdot x_t')(w^*\cdot x_t+w^*\cdot x_t')}{(w_t \cdot x_t)(w_t \cdot x_t')}.
\end{align*}

Also note that $\Delta_t \cdot w_t =0$ regardless of $y_t$.  Hence,
$\E[ \Delta_t \cdot w_t] =0$.  Combining these with the fact that we have shown that $\Delta_t \geq 2\eta \nabla_t \cdot(w_t-w^*)-\eta^2 G^2$, gives,

\begin{align*}
\E[\Delta_t] &\geq 2\eta Z_t (p_t(w^*)-p_t(w_t))^2-\eta^2G^2\\
&\geq 2 \eta \frac{w^*\cdot x_t + w^* \cdot x_t'}{w_t\cdot x_t + w_t \cdot x_t'}\frac{(p_t(w_t)-p_t(w^*))^2}{p_t(w_t)(1-p_t(w_t))}-\eta^2G\\
&\geq 2\eta\alpha \frac{(p_t(w_t)-p_t(w^*))^2}{p_t(w_t)(1-p_t(w_t))}-\eta^2G.
\end{align*}
In the last line we have used the fact that $w\cdot x \in [\alpha,1]$ for $w\in W,x\in X$.
Now, by Lemma \ref{lem:appx1} which follows this proof,
$$\ell_t(w_t)-\ell_t(w^*) \leq \frac{(p_t(w_t)-p_t(w^*))^2}{p_t(w_t)(1-p_t(w_t))}.$$
Combining the previous two displayed equations gives,
$$\E[\Delta_t]  \geq 2 \eta\alpha (\ell_t(w_t)-\ell_t(w^*))-\eta^2G.$$
Finally, since the potential $(w_t -w^*)^2>0$, we have $\sum \Delta_t \leq (w_0-w^*)^2 \leq 4.$  Hence,
$$\sum_t \ell_t(w_t)-\ell_t(w^*) \leq \frac{T \eta^2 G + 4}{2\eta \alpha}.$$
Substituting $\eta=\sqrt{2\alpha/T}$ and $G=2/\alpha$ gives the lemma.
\end{proof}



\begin{lemma}\label{lem:appx1}
Let $p+q=1$ and $p^*+q^*=1$ for $p,p^* \in [0,1]$.  Then,
$$p^* \log \frac{p^*}{p} + q^* \log \frac{q^*}{q} \leq \frac{(p-p^*)^2}{pq}.$$
\end{lemma}
\begin{proof}
By concavity of $\log$, Jensen's inequality implies,
$$p^* \log \frac{p^*}{p} + q^* \log \frac{q^*}{q}  \leq \log \frac{(p^*)^2}{p} + \frac{(q^*)^2}{q}.$$
Simple algebraic manipulation shows that,
$$\frac{(p^*)^2}{p} + \frac{(q^*)^2}{q} = 1 + \frac{(p-p^*)^2}{pq}.$$
Finally, the fact that $\log 1+x \leq x$ completes the lemma.
\end{proof}



\bibliography{sim}
\bibliographystyle{icml2011}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz,
% slightly modified from the 2009 version by Kiri Wagstaff and
% Sam Roweis's 2008 version, which is slightly modified from
% Prasad Tadepalli's 2007 version which is a lightly
% changed version of the previous year's version by Andrew Moore,
% which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.


